import numpy as np
import tensorflow.compat.v1 as tf
import gym
import time
import core as core
from tensorflow.python.framework import graph_util
from tensorflow.python.platform import gfile

def mpi_avg(x):
    """Average a scalar or vector over MPI processes."""
    return x
    #return mpi_sum(x) / num_procs()
    
def mpi_statistics_scalar(x, with_min_and_max=False):
    """
    Get mean/std and optional min/max of scalar x across MPI processes.

    Args:
        x: An array containing samples of the scalar to produce statistics
            for.

        with_min_and_max (bool): If true, return min and max of x in 
            addition to mean and std.
    """
    # x = np.array(x, dtype=np.float32)

    # global_sum, global_n = mpi_sum([np.sum(x), len(x)])
    # mean = global_sum / global_n

    # global_sum_sq = mpi_sum(np.sum((x - mean)**2))
    # std = np.sqrt(global_sum_sq / global_n)  # compute global std

    if with_min_and_max:
        
        return np.mean(x), np.std(x), np.min(x), np.max(x)
    return np.mean(x), np.std(x)

class PPOBuffer:
    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):
        self.obs_buf = np.zeros(core.combine_shape(size, obs_dim), dtype=np.float32)
        self.act_buf = np.zeros(core.combine_shape(size, act_dim), dtype=np.float32)
        self.adv_buf = np.zeros(size, dtype=np.float32)
        self.rew_buf = np.zeros(size, dtype=np.float32)
        self.ret_buf = np.zeros(size, dtype=np.float32)
        self.val_buf = np.zeros(size, dtype=np.float32)
        self.logp_buf = np.zeros(size, dtype=np.float32)
        self.gamma, self.lam = gamma, lam
        self.ptr, self.path_start_idx, self.max_size = 0, 0, size
    
    def store(self, obs, act, rew, val, logp):
        assert self.ptr < self.max_size
        self.obs_buf[self.ptr] = obs
        self.act_buf[self.ptr] = act
        self.rew_buf[self.ptr] = rew
        self.val_buf[self.ptr] = val
        self.logp_buf[self.ptr] = logp
        self.ptr += 1
    
    def finish_path(self, last_val=0):
        path_slice = slice(self.path_start_idx, self.ptr)
        rews = np.append(self.rew_buf[path_slice], last_val)
        vals = np.append(self.val_buf[path_slice], last_val)

        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]
        self.adv_buf[path_slice] = core.discount_cumsum(deltas, self.gamma*self.lam)
        self.ret_buf[path_slice] = core.discount_cumsum(rews, self.gamma)[:-1]
        self.path_start_idx = self.ptr
    
    def get(self):
        assert self.ptr == self.max_size
        self.ptr, self.path_start_idx = 0, 0
        adv_mean, adv_std = mpi_statistics_scalar(self.adv_buf)
        self.adv_buf = (self.adv_buf - adv_mean) / adv_std
        return [self.obs_buf, self.act_buf, self.adv_buf, self.ret_buf, self.logp_buf]


def ppo(env_fn, actor_critic=core.mlp_actor_critic, ac_kwargs=dict(), seed=0,
        steps_per_epoch=4000, epochs=50, gamma=0.99, clip_ratio=0.2, pi_lr=3e-4,
        vf_lr=1e-3, train_pi_iters=80, train_v_iters=80, lam=0.97, max_ep_len=1000,
        target_kl=0.01, save_freq=10):

    #seed += 10000*proc_id()
    tf.set_random_seed(seed)
    np.random.seed(seed)
    
    env = env_fn()
    obs_dim = env.observation_space.shape
    act_dim = env.action_space.shape

    ac_kwargs['action_space'] = env.action_space
    x_ph, a_ph = core.placeholders_from_spaces(env.observation_space, env.action_space)
    adv_ph, ret_ph, logp_old_ph = core.placeholders(None, None, None)

    pi, logp, logp_pi, v = actor_critic(x_ph, a_ph, **ac_kwargs)

    all_phs = [x_ph, a_ph, adv_ph, ret_ph, logp_old_ph]

    get_action_ops = [pi, v, logp_pi]
    
    local_steps_per_epoch = steps_per_epoch
    buf = PPOBuffer(obs_dim, act_dim, local_steps_per_epoch, gamma, lam)
    var_counts = tuple(core.count_vars(scope) for scope in ['pi', 'v'])

    ratio = tf.exp(logp - logp_old_ph)
    min_adv = tf.where(adv_ph>0, (1+clip_ratio)*adv_ph, (1-clip_ratio)*adv_ph)
    pi_loss = -tf.reduce_mean(tf.minimum(ratio*adv_ph, min_adv))
    v_loss = tf.reduce_mean((ret_ph - v)**2)

    approx_kl = tf.reduce_mean(logp_old_ph - logp)
    approx_ent = tf.reduce_mean(-logp)
    clipped = tf.logical_or(ratio > (1+clip_ratio), ratio < (1-clip_ratio))
    clipfrac = tf.reduce_mean(tf.cast(clipped, tf.float32))

    train_pi = tf.train.AdamOptimizer(learning_rate=pi_lr).minimize(pi_loss)
    train_v = tf.train.AdamOptimizer(learning_rate=vf_lr).minimize(v_loss)
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    sess=tf.Session(config=config)
    #sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    #print('after init')
    #sess.run(sync_all_params())

    def update():
        inputs = {k:v for k, v in zip(all_phs, buf.get())}
        pi_l_old, v_l_old, ent = sess.run([pi_loss, v_loss, approx_ent], feed_dict=inputs)

        for i in range(train_pi_iters):
            _, kl = sess.run([train_pi, approx_kl], feed_dict=inputs)
            kl = mpi_avg(kl)
            if kl > 1.5*target_kl:
                #logger.log('Early stopping at step %d due to reaching max kl.'%i)
                break

        for _ in range(train_v_iters):
            sess.run(train_v, feed_dict=inputs)
        

        pi_l_new, v_l_new, kl, cf = sess.run([pi_loss, v_loss, approx_kl, clipfrac], feed_dict=inputs)
        
    start_time = time.time()
    o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0
    o = o[0]
    #print('x_ph is', x_ph)
    for epoch in range(epochs):
        avg_ep_ret, lens = 0, 0
        for t in range(local_steps_per_epoch):
            a, v_t, logp_t = sess.run(get_action_ops, feed_dict={x_ph: o.reshape(1, -1)})
            #print('action is', a)
            o2, r, d, _, _ = env.step(a[0])
            
            ep_ret += r
            ep_len += 1

            buf.store(o, a, r, v_t, logp_t)
            #logger.store(VVals=v_t)
            o = o2

            terminal = d or (ep_len == max_ep_len)
            if terminal or (t==local_steps_per_epoch-1):
                if not(terminal):
                    print('Warning: trajectory cut off by epoch at %d steps.'%ep_len)
                last_val = 0 if d else sess.run(v, feed_dict={x_ph: o.reshape(1, -1)})
                buf.finish_path(last_val)
                if terminal:
                    pass
                    #logger.store(EpRet=ep_ret, EpLen=ep_len)
                avg_ep_ret += ep_ret + last_val
                lens += 1
                o, ep_ret, ep_len = env.reset(), 0, 0
                o = o[0]
        print(avg_ep_ret / lens)
        # if (epoch % save_freq == 0) or (epoch == epochs-1):
        #     logger.save_state({'env': env}, None)
        
        update()
    constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['pi/Squeeze'])
    pb_file_path = '/root/autodl-tmp/spinup/'
    with tf.gfile.FastGFile(pb_file_path+'model.pb', mode='wb') as f:
        f.write(constant_graph.SerializeToString())
        # pi/Squeeze:0
        # action [0]

def testppo(env_fn, seed=0):
    print('in test round')
    sess = tf.Session()
    tf.set_random_seed(seed)
    np.random.seed(seed)
    
    env = env_fn()
    pb_file_path = '/root/autodl-tmp/spinup/'
    
    with gfile.FastGFile(pb_file_path+'model.pb', 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        sess.graph.as_default()
        tf.import_graph_def(graph_def, name='') # 导入计算图
    
    x_ph = sess.graph.get_tensor_by_name('Placeholder:0') #core.placeholders_from_spaces(env.observation_space)
    o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0
    o = o[0]
    get_action_ops = sess.graph.get_tensor_by_name('pi/Squeeze:0')
    while not d:
        a = sess.run(get_action_ops, feed_dict={x_ph: o.reshape(1, -1)})
            #print('action is', a)
        o2, r, d, _, _ = env.step(a[0])
        o = o2
        ep_ret += r
        print(o, ep_ret, a)
    print(ep_ret)

if __name__ == '__main__':
    import os
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--env', type=str, default='CartPole-v1')
    parser.add_argument('--hid', type=int, default=64)
    parser.add_argument('--l', type=int, default=2)
    parser.add_argument('--gamma', type=float, default=0.99)
    parser.add_argument('--seed', '-s', type=int, default=0)
    parser.add_argument('--cpu', type=int, default=0)
    parser.add_argument('--steps', type=int, default=2000)
    parser.add_argument('--epochs', type=int, default=30)
    parser.add_argument('--exp_name', type=str, default='ppo')
    args = parser.parse_args()
    #mpi_fork(args.cpu)
    # from utils.run_utils import setup_logger_kwargs
    # logger_kwargs = setup_logger_kwargs(args.exp_name, args.seed)
    
    ppo(lambda : gym.make(args.env), actor_critic=core.mlp_actor_critic,
        ac_kwargs=dict(hidden_sizes=[args.hid]*args.l), gamma=args.gamma, 
        seed=args.seed, steps_per_epoch=args.steps, epochs=args.epochs)
    #testppo(env_fn=lambda : gym.make(args.env))
